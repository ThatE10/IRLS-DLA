# -*- coding: utf-8 -*-
"""IRLS-sparse regularization

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wSU3onh2A1xwcaLTRRoYdVy_ogyfcXYN

We generally are trying to solve the problem for $Ax=y$, where A is a dictionary and X is some some sparse vector subject to ||x||_1 and our observations are noisy.  We start with the following empirical observation that the solution x* can be given by the following least square problem.

Goal: $x := argmin_{z\in{F(y)}}||z||_{l1^N}$

IRLS Formulation of our goal (equation 1.4)

$x^* := argmin_{z\in{F(y)}}||z||_{l2(w^N)}$

where $w_j:=|x_j^*|^-1$

$F(y):=A^{-1}y$
"""

import torch

"""How does this relate to our problem?  We have noisy observations, $y$,  which we are trying to find sparse codes, $x$, for based off our design matrix, $A$.  """

import numpy as np
from typing import Optional, Tuple
def generate_data(n_features: int = 64,
                  n_components: int = 256,
                  n_samples: int = 2000,
                  sparsity: int = 8,
                  noise_std: float = 0.05) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    Generate synthetic sparse coding data.

    Returns:
        Y: Noisy observations (n_features, n_samples)
        D_true: True dictionary (n_features, n_components)
        X_true: True sparse codes (n_components, n_samples)
    """
    # True dictionary
    D_true = np.random.randn(n_features, n_components)
    D_true /= np.linalg.norm(D_true, axis=0, keepdims=True)

    # Sparse codes
    X_true = np.zeros((n_components, n_samples))
    for i in range(n_samples):
        indices = np.random.choice(n_components, sparsity, replace=False)
        X_true[indices, i] = np.random.randn(sparsity)

    # Noisy observations
    Y_clean = D_true @ X_true
    noise = noise_std * np.random.randn(n_features, n_samples)
    Y = Y_clean + noise

    print(f"Generated data:")
    print(f"  Y shape: {Y.shape}")
    print(f"  D_true shape: {D_true.shape}")
    print(f"  X_true shape: {X_true.shape}")
    print(f"  Sparsity: {sparsity}")
    print(f"  Noise std: {noise_std}")
    print(f"  SNR: {10 * np.log10(np.var(Y_clean) / np.var(noise)):.2f} dB\n")

    return Y, D_true, X_true

Y, A, X = generate_data(n_features=64,n_components=256,n_samples=4000)

#IRLS simple implementation as laid in equation 1.4

print(Y.T[0].shape)
print(X.T[0].shape)

A = torch.Tensor(A)
y=torch.Tensor(Y.T[0])
x_solution=torch.Tensor(X.T[0])
W_0 = torch.diag(torch.rand(256))

prev_x = torch.zeros(256)
W = W_0

for _ in range(5):
  # $x^* := argmin_{z\in{F(y)}}||z||_{l2(w^N)}$
  # x^*=W^{-1}A^T(AW^{−1}A^T)^{−1}b

  x_opt = torch.linalg.solve(A @ torch.linalg.inv(W) @ A.T, y)
  x = torch.linalg.inv(W) @ A.T @ x_opt
  #x = .5*x+.5*prev_x
  prev_x = x

  print(f"Sparsity Norm: {torch.norm(x,0):.2f}")
  print(f"Distance from solution: {torch.norm(x-x_solution):.2f}\n")
  W = torch.diag(x_solution**-1)

"""The results from here suggest that IRLS is very poor at convergence and based off of empirical analysis this is a bad method from this basic approach.  We are getting stuck in local minimums of setting some values to 0.  On page 4 of the paper it suggest a solution where we change what the weight values at the iteration with 0 values to get out of the local minimum.  """

print(A.shape)
print(torch.norm(x_solution))
print(torch.norm(x))

#IRLS simple implementation with a geometric mean majorization of epsilon.  As laid out in algorithm 1


print(Y.T[0].shape)
print(X.T[0].shape)

A = torch.Tensor(A)
y=torch.Tensor(Y.T[0])
x_solution=torch.Tensor(X.T[0])

W_0 = torch.diag(torch.ones(256))
W = W_0
epsilon = 1.0
N = 200
for _ in range(N):
  # $x^* := argmin_{z\in{F(y)}}||z||_{l2(w^N)}$
  # x^*=W^{-1}A^T(AW^{−1}A^T)^{−1}b

  x_opt = torch.linalg.solve(A @ torch.linalg.inv(W) @ A.T, y)
  x = torch.linalg.inv(W) @ A.T @ x_opt

  print(f"Sparsity Norm: {torch.norm(x,0):.2f}")
  print(f"Distance from solution: {torch.norm(x-x_solution):.2f}\n")
  W = torch.diag((x**2 + (epsilon * torch.ones(256))**2)**-.5)

  epsilon = min(epsilon, max(torch.abs(x))/N)

print(f"Sparsity Norm: {torch.norm(x,0):.2f}")
print(f"Sparsity Norm: {torch.norm(x_solution,0):.2f}")

torch.norm(A@torch.rand(256)-A@x_solution)

torch.argmax(x_solution)

x[torch.abs(x)<0.1]=0

print(f"Distance from solution: {torch.norm(x-x_solution):.2f}\n")

x_solution

x

print(f"Sparsity Norm: {torch.norm(x,0):.2f}")
print(f"Sparsity Norm: {torch.norm(x_solution,0):.2f}")

