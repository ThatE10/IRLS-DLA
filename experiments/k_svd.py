# -*- coding: utf-8 -*-
"""K-SVD.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ejf9d37XHyiB2zpBogA-LkMTIL0ZH29O
"""



"""Generate dummy data given sparse dictionary D \in R ^ n x m, we hae codes X \in R^m ||X||_0 = 1 such.  We generate data vector R^n, where n << m

Dataset = Y_i = Dx + N(0,1)
"""



"""1.  


2.  [GPU parallel implementation of the approximate K-SVD algorithm using OpenCL](https://ieeexplore.ieee.org/document/6952053)

3.  

4.  

https://github.com/IdanBanani/Orthogonal-Matching-Pursuit--OMP--and-Batch-OMP-algorithm-/blob/master/batch_omp_cuda.cu

"""

try:
    import cupy as cp
    HAS_CUPY = True
except ImportError:
    HAS_CUPY = False
    import numpy as cp  # Fallback for type hinting or simple operations, though PAKSVD will fail

import numpy as np
from typing import Tuple, Optional

class PAKSVD:
    """
    Parallel Approximate K-SVD implementation using CuPy for GPU parralelism.
    """

    def __init__(self, n_components: int, n_nonzero_coefs: int,
                 max_iter: int = 200, n_parallel_atoms: Optional[int] = None,
                 n_update_cycles: int = 1, tol: float = 1e-6):
        """
        Initialize PAK-SVD.

        Args:
            n_components: Number of dictionary atoms (n)
            n_nonzero_coefs: Target sparsity level (s)
            max_iter: Number of K-SVD iterations (K)
            n_parallel_atoms: Number of atoms to update in parallel (Ã±)
                            If None, defaults to n_components (fully parallel)
            n_update_cycles: Number of dictionary update cycles per iteration (u)
            tol: Convergence tolerance
        """
        self.n_components = n_components
        self.n_nonzero_coefs = n_nonzero_coefs
        self.max_iter = max_iter
        self.n_parallel_atoms = n_parallel_atoms or n_components
        self.n_update_cycles = n_update_cycles
        self.tol = tol
        self.dictionary_ = None
        self.error_history_ = []

    def fit(self, Y: np.ndarray, init_dict: Optional[np.ndarray] = None) -> 'PAKSVD':
        """
        Train dictionary on signal set Y.

        Args:
            Y: Training signals of shape (p, m) where p is signal dimension
               and m is number of signals
            init_dict: Initial dictionary of shape (p, n). If None, randomly initialize

        Returns:
            self
        """
        # Transfer data to GPU
        Y_gpu = cp.asarray(Y, dtype=cp.float32)
        p, m = Y_gpu.shape

        # Initialize dictionary
        if init_dict is None:
            self.dictionary_ = cp.random.randn(p, self.n_components).astype(cp.float32)
            # Normalize atoms
            self.dictionary_ /= cp.linalg.norm(self.dictionary_, axis=0, keepdims=True)
        else:
            self.dictionary_ = cp.asarray(init_dict, dtype=cp.float32)

        # Main K-SVD iterations
        for k in range(self.max_iter):
            # Stage 1: Compute sparse representations
            X = self._sparse_coding(Y_gpu)

            # Stage 2: Update dictionary atoms (with multiple cycles)
            for u in range(self.n_update_cycles):
                self._update_dictionary(Y_gpu, X)

            # Compute and store approximation error
            E = Y_gpu - self.dictionary_ @ X
            rmse = cp.sqrt(cp.mean(E ** 2)).item()
            self.error_history_.append(rmse)

            if k > 0 and abs(self.error_history_[-2] - self.error_history_[-1]) < self.tol:
                print(f"Converged at iteration {k}")
                break

            if (k + 1) % 10 == 0:
                print(f"Iteration {k+1}/{self.max_iter}, RMSE: {rmse:.6f}")

        return self

    def _sparse_coding(self, Y: cp.ndarray) -> cp.ndarray:
        """
        Compute sparse representations using Batch OMP.

        Args:
            Y: Signal matrix (p, m)

        Returns:
            X: Sparse representation matrix (n, m)
        """
        p, m = Y.shape
        n = self.n_components
        s = self.n_nonzero_coefs

        # Precompute gram matrices (steps 2-3 in Algorithm 1)
        G = self.dictionary_.T @ self.dictionary_  # (n, n)
        H = self.dictionary_.T @ Y  # (n, m)

        # Initialize sparse representations
        X = cp.zeros((n, m), dtype=cp.float32)

        # Process signals in batches for memory efficiency
        batch_size = min(1024, m)
        n_batches = (m + batch_size - 1) // batch_size

        for batch_idx in range(n_batches):
            start_idx = batch_idx * batch_size
            end_idx = min(start_idx + batch_size, m)
            batch_H = H[:, start_idx:end_idx]

            # Run OMP for each signal in batch
            for j in range(end_idx - start_idx):
                X[:, start_idx + j] = self._omp_single(G, batch_H[:, j], s)

        return X

    def _omp_single(self, G: cp.ndarray, h: cp.ndarray, s: int) -> cp.ndarray:
        """
        Orthogonal Matching Pursuit for a single signal (Batch OMP).

        Args:
            G: Gram matrix D^T D of shape (n, n)
            h: Correlations D^T y of shape (n,)
            s: Target sparsity

        Returns:
            x: Sparse representation of shape (n,)
        """
        n = len(h)
        x = cp.zeros(n, dtype=cp.float32)

        residual_corr = h.copy()
        indices = []
        L = cp.zeros((s, s), dtype=cp.float32)  # Cholesky factor

        for iter_idx in range(s):
            # Find atom with maximum correlation
            idx = cp.argmax(cp.abs(residual_corr)).item()
            indices.append(idx)

            # Update Cholesky decomposition
            if iter_idx == 0:
                L[0, 0] = cp.sqrt(G[idx, idx])
            else:
                # Solve L * w = G[indices[:-1], idx]
                w = cp.zeros(iter_idx, dtype=cp.float32)
                for i in range(iter_idx):
                    w[i] = G[indices[i], idx]
                    for j in range(i):
                        w[i] -= L[i, j] * w[j]
                    w[i] /= L[i, i]

                L[iter_idx, :iter_idx] = w
                L[iter_idx, iter_idx] = cp.sqrt(G[idx, idx] - cp.sum(w ** 2))

            # Solve for coefficients: L * L^T * x_I = h_I
            h_I = h[indices]
            # Forward substitution: L * y = h_I
            y = cp.zeros(iter_idx + 1, dtype=cp.float32)
            for i in range(iter_idx + 1):
                y[i] = h_I[i]
                for j in range(i):
                    y[i] -= L[i, j] * y[j]
                y[i] /= L[i, i]

            # Backward substitution: L^T * x_I = y
            x_I = cp.zeros(iter_idx + 1, dtype=cp.float32)
            for i in range(iter_idx, -1, -1):
                x_I[i] = y[i]
                for j in range(i + 1, iter_idx + 1):
                    x_I[i] -= L[j, i] * x_I[j]
                x_I[i] /= L[i, i]

            # Update residual correlation
            for i, idx_i in enumerate(indices):
                x[idx_i] = x_I[i]

            residual_corr = h - G @ x

        return x

    def _update_dictionary(self, Y: cp.ndarray, X: cp.ndarray):
        """
        Update dictionary atoms in parallel using power method.

        Args:
            Y: Signal matrix (p, m)
            X: Sparse representation matrix (n, m)
        """
        n = self.n_components
        n_tilde = self.n_parallel_atoms
        n_groups = (n + n_tilde - 1) // n_tilde

        for group_idx in range(n_groups):
            # Compute current error (step 9)
            E = Y - self.dictionary_ @ X

            # Process n_tilde atoms in parallel
            start_atom = group_idx * n_tilde
            end_atom = min(start_atom + n_tilde, n)

            for j in range(start_atom, end_atom):
                # Find signals using this atom (step 11)
                indices = cp.where(cp.abs(X[j, :]) > 1e-10)[0]

                if len(indices) == 0:
                    continue

                # Compute F = E_I + D_j * X_{j,I} (step 12)
                F = E[:, indices] + cp.outer(self.dictionary_[:, j], X[j, indices])

                # Power method update (step 13)
                new_atom = F @ X[j, indices]
                norm = cp.linalg.norm(new_atom)

                if norm > 1e-10:
                    self.dictionary_[:, j] = new_atom / norm

                    # Update coefficients (step 14)
                    X[j, indices] = self.dictionary_[:, j].T @ F

    def transform(self, Y: np.ndarray) -> np.ndarray:
        """
        Compute sparse representations for new signals.

        Args:
            Y: Signal matrix (p, m)

        Returns:
            X: Sparse representation matrix (n, m)
        """
        Y_gpu = cp.asarray(Y, dtype=cp.float32)
        X_gpu = self._sparse_coding(Y_gpu)
        return cp.asnumpy(X_gpu)

    def get_dictionary(self) -> np.ndarray:
        """Get the learned dictionary as numpy array."""
        return cp.asnumpy(self.dictionary_)

    def reconstruct(self, Y: np.ndarray) -> np.ndarray:
        """
        Reconstruct signals from sparse representations.

        Args:
            Y: Signal matrix (p, m)

        Returns:
            Y_reconstructed: Reconstructed signals (p, m)
        """
        X = self.transform(Y)
        X_gpu = cp.asarray(X, dtype=cp.float32)
        Y_recon = self.dictionary_ @ X_gpu
        return cp.asnumpy(Y_recon)



def generate_sparse_coding_data(
    n: int = 64,           # Signal dimension (n << m)
    m: int = 256,          # Dictionary size (overcomplete)
    n_samples: int = 5000, # Number of training samples
    sparsity: int = 5,     # Number of non-zero coefficients (||X||_0)
    noise_std: float = 0.1 # Standard deviation of Gaussian noise
) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    Generate synthetic data for overcomplete dictionary learning.

    Model: Y_i = D @ X_i + noise, where noise ~ N(0, noise_std^2)

    Args:
        n: Signal dimension (number of features)
        m: Dictionary size (number of atoms), should be m > n for overcomplete
        n_samples: Number of training signals to generate
        sparsity: Target sparsity level (||X||_0, number of non-zero coefficients)
        noise_std: Standard deviation of additive Gaussian noise

    Returns:
        Y: Data matrix of shape (n, n_samples) - noisy observations
        D_true: True dictionary of shape (n, m) - normalized atoms
        X_true: True sparse codes of shape (m, n_samples)
    """
    assert m > n, "Dictionary must be overcomplete (m > n)"
    assert sparsity <= m, "Sparsity must be <= dictionary size"

    D_true = np.random.randn(n, m)

    D_true = D_true / np.linalg.norm(D_true, axis=0, keepdims=True)

    X_true = np.zeros((m, n_samples))

    for i in range(n_samples):
        # Randomly select 'sparsity' indices for non-zero coefficients
        nonzero_indices = np.random.choice(m, sparsity, replace=False)

        # Generate random coefficients (e.g., from standard normal)
        coefficients = np.random.randn(sparsity)

        # Optionally scale coefficients for variety
        coefficients *= np.random.uniform(0.5, 2.0, sparsity)

        X_true[nonzero_indices, i] = coefficients

    # Generate clean signals: Y_clean = D @ X
    Y_clean = D_true @ X_true

    # Add Gaussian noise: Y = Y_clean + N(0, noise_std^2 * I)
    noise = noise_std * np.random.randn(n, n_samples)
    Y = Y_clean + noise

    print(f"Generated data:")
    print(f"  Signal dimension (n): {n}")
    print(f"  Dictionary size (m): {m} (overcomplete factor: {m/n:.1f}x)")
    print(f"  Number of samples: {n_samples}")
    print(f"  Sparsity level: {sparsity}")
    print(f"  Noise std: {noise_std}")
    print(f"  Y shape: {Y.shape}")
    print(f"  D_true shape: {D_true.shape}")
    print(f"  X_true shape: {X_true.shape}")
    print(f"  Average sparsity: {np.mean(np.sum(np.abs(X_true) > 1e-10, axis=0)):.1f}")
    print(f"  SNR: {10 * np.log10(np.var(Y_clean) / np.var(noise)):.2f} dB")

    return Y, D_true, X_true

# Generate data
Y, D_true, X_true = generate_sparse_coding_data(
    n=64,           # Signal dimension
    m=256,          # Overcomplete dictionary
    n_samples=5000, # Training samples
    sparsity=8,     # 8 non-zero coefficients
    noise_std=0.05  # Low noise
)

# Verify properties
print(f"\nVerification:")
print(f"  D is normalized: {np.allclose(np.linalg.norm(D_true, axis=0), 1.0)}")
print(f"  X is sparse: {np.sum(np.abs(X_true) > 1e-10) / X_true.size * 100:.2f}% non-zero")

# Reconstruction error without noise
Y_reconstructed = D_true @ X_true
reconstruction_error = np.linalg.norm(Y - Y_reconstructed, 'fro') / np.sqrt(Y.size)
print(f"  RMSE (should match noise_std): {reconstruction_error:.4f}")

# Example: Use with PAK-SVD
print("\n" + "="*50)
print("Training PAK-SVD on generated data...")
print("="*50)


if HAS_CUPY:
    paksvd = PAKSVD(
        n_components=256,      # Match m
        n_nonzero_coefs=8,     # Match sparsity
        max_iter=100,
        n_parallel_atoms=64,
        n_update_cycles=1
    )

    try:
        paksvd.fit(Y)

        # Test reconstruction
        Y_pred = paksvd.reconstruct(Y)
        final_rmse = np.sqrt(np.mean((Y - Y_pred) ** 2))
        print(f"\nFinal reconstruction RMSE: {final_rmse:.6f}")
        print(f"Improvement over noise baseline: {(reconstruction_error - final_rmse) / reconstruction_error * 100:.1f}%")

    except Exception as e:
        print(f"Note: PAK-SVD training requires CuPy/GPU. Error: {e}")
else:
    print("\nSkipping PAK-SVD example because CuPy is not installed.")

# import cupy as cp (already imported at top)

if HAS_CUPY:
    # Load the CUDA kernel
    try:
        with open('omp_kernel.cu', 'r') as f:
            kernel_code = f.read()

        mod = cp.RawModule(code=kernel_code)
        kernel = mod.get_function('omp_kernel_function')

        # Prepare data
        X = cp.random.randn(100, 50)  # Example data
        y = cp.random.randn(100)      # Example target

        # Launch the kernel
        kernel((X.shape[0],), (256,), (X, y))
    except FileNotFoundError:
        print("omp_kernel.cu not found, skipping kernel test.")
    except Exception as e:
        print(f"Kernel execution failed: {e}")

import numpy as np
from typing import Optional, Tuple

class SimpleKSVD:
    """
    Simple K-SVD dictionary learning algorithm using NumPy (CPU only).

    Based on: "K-SVD: An Algorithm for Designing Overcomplete Dictionaries
    for Sparse Representation" by Aharon, Elad, and Bruckstein (2006)
    """

    def __init__(self, n_components: int, n_nonzero_coefs: int,
                 max_iter: int = 100, tol: float = 1e-6, verbose: bool = True):
        """
        Initialize K-SVD.

        Args:
            n_components: Number of dictionary atoms (n)
            n_nonzero_coefs: Target sparsity level (s)
            max_iter: Maximum number of iterations
            tol: Convergence tolerance
            verbose: Print progress
        """
        self.n_components = n_components
        self.n_nonzero_coefs = n_nonzero_coefs
        self.max_iter = max_iter
        self.tol = tol
        self.verbose = verbose
        self.dictionary_ = None
        self.error_history_ = []

    def fit(self, Y: np.ndarray, init_dict: Optional[np.ndarray] = None) -> 'SimpleKSVD':
        """
        Train dictionary on signal set Y.

        Args:
            Y: Training signals of shape (n_features, n_samples)
            init_dict: Initial dictionary. If None, randomly initialize

        Returns:
            self
        """
        n_features, n_samples = Y.shape

        # Initialize dictionary
        if init_dict is None:
            # Random initialization
            self.dictionary_ = np.random.randn(n_features, self.n_components)
            self._normalize_dictionary()
        else:
            self.dictionary_ = init_dict.copy()
            self._normalize_dictionary()

        if self.verbose:
            print(f"K-SVD: Training on {n_samples} signals")
            print(f"Dictionary: {n_features} x {self.n_components}, Sparsity: {self.n_nonzero_coefs}")

        # Main K-SVD loop
        for iteration in range(self.max_iter):
            # Stage 1: Sparse Coding (find X given D)
            X = self._sparse_coding(Y)

            # Stage 2: Dictionary Update (update D given X)
            self._update_dictionary(Y, X)

            # Compute approximation error
            error = np.linalg.norm(Y - self.dictionary_ @ X, 'fro') / np.sqrt(Y.size)
            self.error_history_.append(error)

            if self.verbose and (iteration + 1) % 10 == 0:
                print(f"Iteration {iteration + 1:3d}/{self.max_iter}: RMSE = {error:.6f}")

            # Check convergence
            if iteration > 0:
                change = abs(self.error_history_[-2] - self.error_history_[-1])
                if change < self.tol:
                    if self.verbose:
                        print(f"Converged at iteration {iteration + 1}")
                    break

        if self.verbose:
            print(f"Training complete. Final RMSE: {self.error_history_[-1]:.6f}")

        return self

    def _sparse_coding(self, Y: np.ndarray) -> np.ndarray:
        """
        Compute sparse representations using Orthogonal Matching Pursuit (OMP).

        Args:
            Y: Signal matrix (n_features, n_samples)

        Returns:
            X: Sparse codes (n_components, n_samples)
        """
        n_samples = Y.shape[1]
        X = np.zeros((self.n_components, n_samples))

        # Run OMP for each signal
        for i in range(n_samples):
            X[:, i] = self._omp(Y[:, i])

        return X

    def _omp(self, y: np.ndarray) -> np.ndarray:
        """
        Orthogonal Matching Pursuit for a single signal.

        Args:
            y: Single signal vector

        Returns:
            x: Sparse representation
        """
        x = np.zeros(self.n_components)
        residual = y.copy()
        indices = []

        for _ in range(self.n_nonzero_coefs):
            # Find atom with maximum correlation
            correlations = self.dictionary_.T @ residual
            idx = np.argmax(np.abs(correlations))
            indices.append(idx)

            # Solve least squares on selected atoms
            D_selected = self.dictionary_[:, indices]
            x_selected = np.linalg.lstsq(D_selected, y, rcond=None)[0]

            # Update sparse code
            x[indices] = x_selected

            # Update residual
            residual = y - self.dictionary_ @ x

        return x

    def _update_dictionary(self, Y: np.ndarray, X: np.ndarray):
        """
        Update dictionary atoms using K-SVD (rank-1 SVD approximation).

        Args:
            Y: Signal matrix (n_features, n_samples)
            X: Sparse codes (n_components, n_samples)
        """
        for k in range(self.n_components):
            # Find signals that use this atom
            indices = np.where(np.abs(X[k, :]) > 1e-10)[0]

            if len(indices) == 0:
                # Atom is unused - reinitialize randomly
                self.dictionary_[:, k] = np.random.randn(self.dictionary_.shape[0])
                self.dictionary_[:, k] /= np.linalg.norm(self.dictionary_[:, k])
                continue

            # Compute error without this atom's contribution
            E_k = Y[:, indices] - self.dictionary_ @ X[:, indices] + \
                  np.outer(self.dictionary_[:, k], X[k, indices])

            # Update atom and coefficients using SVD (rank-1 approximation)
            U, s, Vt = np.linalg.svd(E_k, full_matrices=False)

            # Update dictionary atom (first left singular vector)
            self.dictionary_[:, k] = U[:, 0]

            # Update coefficients (first singular value * first right singular vector)
            X[k, indices] = s[0] * Vt[0, :]

    def _normalize_dictionary(self):
        """Normalize all dictionary atoms to unit norm."""
        norms = np.linalg.norm(self.dictionary_, axis=0)
        norms[norms == 0] = 1  # Avoid division by zero
        self.dictionary_ /= norms

    def transform(self, Y: np.ndarray) -> np.ndarray:
        """
        Compute sparse representations for new signals.

        Args:
            Y: Signal matrix (n_features, n_samples)

        Returns:
            X: Sparse codes (n_components, n_samples)
        """
        return self._sparse_coding(Y)

    def reconstruct(self, Y: np.ndarray) -> np.ndarray:
        """
        Reconstruct signals from their sparse representations.

        Args:
            Y: Signal matrix (n_features, n_samples)

        Returns:
            Y_reconstructed: Reconstructed signals
        """
        X = self.transform(Y)
        return self.dictionary_ @ X

    def get_dictionary(self) -> np.ndarray:
        """Get the learned dictionary."""
        return self.dictionary_.copy()


# Simple data generation function
def generate_data(n_features: int = 64,
                  n_components: int = 256,
                  n_samples: int = 2000,
                  sparsity: int = 8,
                  noise_std: float = 0.05) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    Generate synthetic sparse coding data.

    Returns:
        Y: Noisy observations (n_features, n_samples)
        D_true: True dictionary (n_features, n_components)
        X_true: True sparse codes (n_components, n_samples)
    """
    # True dictionary
    D_true = np.random.randn(n_features, n_components)
    D_true /= np.linalg.norm(D_true, axis=0, keepdims=True)

    # Sparse codes
    X_true = np.zeros((n_components, n_samples))
    for i in range(n_samples):
        indices = np.random.choice(n_components, sparsity, replace=False)
        X_true[indices, i] = np.random.randn(sparsity)

    # Noisy observations
    Y_clean = D_true @ X_true
    noise = noise_std * np.random.randn(n_features, n_samples)
    Y = Y_clean + noise

    print(f"Generated data:")
    print(f"  Y shape: {Y.shape}")
    print(f"  D_true shape: {D_true.shape}")
    print(f"  X_true shape: {X_true.shape}")
    print(f"  Sparsity: {sparsity}")
    print(f"  Noise std: {noise_std}")
    print(f"  SNR: {10 * np.log10(np.var(Y_clean) / np.var(noise)):.2f} dB\n")

    return Y, D_true, X_true


# Example usage and simple tests
if __name__ == "__main__":
    print("="*70)
    print("SIMPLE K-SVD EXAMPLE")
    print("="*70 + "\n")

    # Set random seed for reproducibility
    np.random.seed(42)

    # Generate synthetic data
    Y, D_true, X_true = generate_data(
        n_features=64,
        n_components=128,
        n_samples=1000,
        sparsity=6,
        noise_std=0.05
    )

    # Train K-SVD
    print("Training K-SVD...\n")
    ksvd = SimpleKSVD(
        n_components=128,
        n_nonzero_coefs=6,
        max_iter=1000,
        verbose=True
    )

    ksvd.fit(Y)

    # Test reconstruction
    print("\n" + "="*70)
    print("TESTING RECONSTRUCTION")
    print("="*70)

    Y_recon = ksvd.reconstruct(Y)
    rmse = np.sqrt(np.mean((Y - Y_recon) ** 2))

    print(f"Reconstruction RMSE: {rmse:.6f}")
    print(f"Noise level: {0.05:.6f}")

    # Test on subset of data
    print("\n" + "="*70)
    print("TESTING ON NEW DATA")
    print("="*70)

    Y_test = Y[:, :100]
    X_test = ksvd.transform(Y_test)
    Y_test_recon = ksvd.dictionary_ @ X_test

    test_rmse = np.sqrt(np.mean((Y_test - Y_test_recon) ** 2))
    test_sparsity = np.mean(np.sum(np.abs(X_test) > 1e-10, axis=0))

    print(f"Test RMSE: {test_rmse:.6f}")
    print(f"Test sparsity: {test_sparsity:.2f}")

    # Verify dictionary properties
    print("\n" + "="*70)
    print("DICTIONARY PROPERTIES")
    print("="*70)

    D = ksvd.get_dictionary()
    norms = np.linalg.norm(D, axis=0)

    print(f"Dictionary shape: {D.shape}")
    print(f"All atoms normalized: {np.allclose(norms, 1.0)}")
    print(f"Atom norms - min: {norms.min():.6f}, max: {norms.max():.6f}")

    # Simple comparison test
    print("\n" + "="*70)
    print("COMPARISON: Different Sparsity Levels")
    print("="*70)

    for s in [4, 6, 8, 10]:
        ksvd_test = SimpleKSVD(
            n_components=128,
            n_nonzero_coefs=s,
            max_iter=30,
            verbose=False
        )
        ksvd_test.fit(Y)

        Y_recon_test = ksvd_test.reconstruct(Y)
        rmse_test = np.sqrt(np.mean((Y - Y_recon_test) ** 2))

        print(f"Sparsity {s:2d}: Final RMSE = {rmse_test:.6f}")

